import os
import sys
import requests

# Add backend path to sys.path
backend_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, backend_path)

from app.chatbot.ChatbotOllama import ChatbotOllama

def chat_with_ollama():
    base_url = "http://localhost:11434"
    model_name = "hf.co/Cactus-Compute/Qwen3-1.7B-Instruct-GGUF:Q4_K_M"

    # Ki·ªÉm tra Ollama server
    try:
        r = requests.get(f"{base_url}/api/tags", timeout=3)
        if not r.json().get("models", []):
            print("‚ùå Ch∆∞a c√≥ model. Ch·∫°y: ollama pull llama2")
            return
    except:
        print("‚ùå Ollama ch∆∞a ch·∫°y. Ch·∫°y: ollama serve")
        return

    # Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng
    os.environ["OLLAMA_URL"] = base_url
    os.environ["OLLAMA_MODEL"] = model_name

    # T·∫°o chatbot
    bot = ChatbotOllama()
    bot.add_system_message(
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch, th√¢n thi·ªán v√† lu√¥n h·ªó tr·ª£ ng∆∞·ªùi d√πng m·ªôt c√°ch t·ªët nh·∫•t. Ch·ªâ tr·∫£ l·ªùi trong tools n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu."
    )

    print(f"‚úÖ Chatbot s·∫µn s√†ng! Model: {model_name}")
    print("G√µ 'quit' ƒë·ªÉ tho√°t.\n")
    tools = bot.get_available_tools()
    print(f"üîß C√¥ng c·ª• s·∫µn c√≥: {', '.join(tools)}\n")

    while True:
        user = input("üë§ You: ").strip()
        if user.lower() in ["quit", "exit"]:
            print("üëã T·∫°m bi·ªát!")
            break
        if not user:
            continue
        resp = bot.chat_with_tools(user, tools, max_steps = 1)
        # Lo·∫°i b·ªè th·∫ª <think> n·∫øu c√≥
        if "<think>" in resp:
            resp = resp.split("</think>")[-1].strip()
        print(f"ü§ñ Bot: {resp}")

if __name__ == "__main__":
    chat_with_ollama()
